"""
Tulsa City Mixin for scrapers that share a common data source.

⚠️ AI-GENERATED CODE - NOT YET REVIEWED BY A HUMAN ⚠️

This file was generated by Claude (Anthropic) as part of the initial
city-scrapers-tulsa repository setup. The CSS selectors and URL patterns
are PLACEHOLDERS based on analysis of the Tulsa website structure but
require testing and adjustment to work correctly.

This mixin provides common scraping logic for Tulsa city agencies that use
the same calendar/meeting system. Individual agencies are identified by a
unique agency_id parameter.

Based on the WichitaCityMixin pattern from city-scrapers-wichita.

Usage:
    Create spider classes by inheriting from TulsaCityMixin and defining:
    - name: Spider slug (e.g., "tulok_city_council")
    - agency: Full agency name (e.g., "Tulsa City Council")
    - agency_id: Agency identifier used in URLs (e.g., "1642")

Example:
    class TulokCityCouncilSpider(TulsaCityMixin):
        name = "tulok_city_council"
        agency = "Tulsa City Council"
        agency_id = "1642"
"""

import re
from datetime import datetime
from urllib.parse import urljoin, urlparse

import scrapy
from city_scrapers_core.constants import BOARD, CITY_COUNCIL, COMMITTEE, NOT_CLASSIFIED
from city_scrapers_core.items import Meeting
from city_scrapers_core.spiders import CityScrapersSpider
from dateutil.parser import parse
from dateutil.relativedelta import relativedelta


class TulsaCityMixinMeta(type):
    """
    Metaclass that enforces the implementation of required static
    variables in child classes that inherit from TulsaCityMixin.
    """

    def __init__(cls, name, bases, dct):
        required_static_vars = ["agency", "name", "agency_id"]
        missing_vars = [var for var in required_static_vars if var not in dct]

        if missing_vars:
            missing_vars_str = ", ".join(missing_vars)
            raise NotImplementedError(
                f"{name} must define the following static variable(s): {missing_vars_str}."  # noqa
            )

        super().__init__(name, bases, dct)


class TulsaCityMixin(CityScrapersSpider, metaclass=TulsaCityMixinMeta):
    """
    Base mixin class for scraping Tulsa city government meetings.

    This class is designed to scrape data from the City of Tulsa government website.
    Boards and committees are identified by a unique 'agency_id' value.

    Required class variables (enforced by metaclass):
        name (str): Spider name/slug (e.g., "tulsa_city_council")
        agency (str): Full agency name (e.g., "Tulsa City Council")
        agency_id (str): Agency identifier for URL filtering (e.g., "council")

    Attributes:
        timezone (str): Timezone for the city (Central Time)
        base_url (str): Base URL for the city website
        links (list): Default links to include in all meetings
    """

    # Required to be overridden (enforced by metaclass)
    name = None
    agency = None
    agency_id = None

    # Common configuration - update these based on actual Tulsa website
    timezone = "America/Chicago"  # Tulsa is in Central Time
    base_url = "https://www.cityoftulsa.org"  # UPDATE WITH ACTUAL URL
    links = [
        {
            "href": "https://www.cityoftulsa.org/meetings",  # UPDATE
            "title": "Tulsa City, meeting materials page",
        }
    ]

    def start_requests(self):
        """
        Generate initial request(s) for the agency's meetings.

        Creates a URL with the agency_id parameter and a date range
        (1 month in the past to 6 months in the future).

        Yields:
            scrapy.Request: Initial request to the calendar page
        """
        now = datetime.now()
        start_date = (now - relativedelta(months=1)).replace(day=1)
        start_date_str = start_date.strftime("%m/%d/%Y")
        end_date = (now + relativedelta(months=6)).replace(day=1)
        end_date_str = end_date.strftime("%m/%d/%Y")

        # TODO: Update URL pattern based on actual Tulsa website structure
        calendar_url = (
            f"{self.base_url}/calendar?"
            f"agency={self.agency_id}&"
            f"start={start_date_str}&"
            f"end={end_date_str}"
        )

        self.logger.info(f"Starting requests for {self.agency} ({self.agency_id})")
        yield scrapy.Request(calendar_url, self.parse)

    def parse(self, response):
        """
        Parse the calendar listing page.

        Extracts meeting links for this agency and follows them to detail pages.

        Args:
            response (scrapy.Response): Response from the calendar page

        Yields:
            scrapy.Request: Requests to individual meeting detail pages
        """
        # TODO: Update selector based on actual HTML structure
        # This selector should find meeting items specific to this agency
        selector = f".agency-{self.agency_id} .meeting-item"

        meeting_items = response.css(selector)
        self.logger.info(f"Found {len(meeting_items)} meetings for {self.agency}")

        for item in meeting_items:
            meeting_url = item.css("a.meeting-link::attr(href)").get()
            if meeting_url:
                yield response.follow(meeting_url, self._parse_detail)
            else:
                self.logger.warning(
                    f"No meeting URL found in item: {item.get()[:100]}"
                )

    def _parse_detail(self, response):
        """
        Parse individual meeting detail page and create Meeting item.

        Args:
            response (scrapy.Response): Response from meeting detail page

        Yields:
            Meeting: Meeting item with all required fields
        """
        title = self._parse_title(response)

        meeting = Meeting(
            title=title,
            description=self._parse_description(response),
            classification=self._parse_classification(title),
            start=self._parse_start(response),
            end=self._parse_end(response),
            all_day=False,
            time_notes="",
            location=self._parse_location(response),
            links=self._parse_links(response),
            source=response.url,
        )

        meeting["status"] = self._get_status(meeting)
        meeting["id"] = self._get_id(meeting)

        yield meeting

    def _parse_title(self, response):
        """
        Extract the meeting title.

        Args:
            response (scrapy.Response): Response from meeting detail page

        Returns:
            str: Meeting title
        """
        # TODO: Update selector based on actual HTML
        title = response.css("h1.meeting-title::text").get()
        if not title:
            self.logger.warning(f"No title found for {response.url}")
            return "Untitled Meeting"
        return title.strip()

    def _parse_description(self, response):
        """
        Extract and clean the meeting description.

        Removes extra whitespace and non-printable characters.
        Includes URLs from links in the description.

        Args:
            response (scrapy.Response): Response from meeting detail page

        Returns:
            str: Cleaned meeting description
        """
        # TODO: Update selector based on actual HTML
        description_sel = response.css("div.meeting-description")
        description_texts = []

        # Extract text nodes
        for text_node in description_sel.css("::text").getall():
            cleaned_text = re.sub(r"\s+", " ", text_node).strip()
            if cleaned_text:
                description_texts.append(cleaned_text)

        # Extract and format embedded links
        for a_tag in description_sel.css("a"):
            text = a_tag.css("::text").get(default="").strip()
            href = a_tag.css("::attr(href)").get(default="").strip()
            if text and href:
                description_texts.append(f"{text}({href})")

        description = " ".join(description_texts).strip()

        # Remove non-printable characters
        description = re.sub(r"[^\x20-\x7E]+", "", description)

        return description

    def _parse_classification(self, title):
        """
        Determine the meeting classification based on title.

        Args:
            title (str): Meeting title

        Returns:
            str: One of BOARD, COMMITTEE, CITY_COUNCIL, or NOT_CLASSIFIED
        """
        if not title:
            return NOT_CLASSIFIED

        title_lower = title.lower()

        if "board" in title_lower:
            return BOARD
        elif "committee" in title_lower:
            return COMMITTEE
        elif "council" in title_lower:
            return CITY_COUNCIL
        else:
            return NOT_CLASSIFIED

    def _parse_start(self, response):
        """
        Extract the meeting start datetime.

        Handles time ranges (e.g., "2:00 PM - 4:00 PM") by taking the first time.

        Args:
            response (scrapy.Response): Response from meeting detail page

        Returns:
            datetime: Naive datetime object (no timezone info)
        """
        # TODO: Update selectors based on actual HTML
        date_str = response.css(".meeting-date::text").get()
        time_str = response.css(".meeting-time::text").get()

        if not date_str or not time_str:
            self.logger.warning(
                f"Missing date or time for meeting at {response.url}"
            )
            # Return a placeholder - consider skipping this meeting instead
            return datetime.now()

        # Handle time ranges (e.g., "2:00 PM - 4:00 PM")
        if " - " in time_str:
            time_str = time_str.split(" - ")[0].strip()

        datetime_str = f"{date_str} {time_str}"

        try:
            return parse(datetime_str)
        except Exception as e:
            self.logger.error(
                f"Error parsing datetime '{datetime_str}' at {response.url}: {e}"
            )
            return datetime.now()

    def _parse_end(self, response):
        """
        Extract the meeting end datetime.

        Returns None if no end time is available.

        Args:
            response (scrapy.Response): Response from meeting detail page

        Returns:
            datetime or None: Naive datetime object or None
        """
        # TODO: Update selectors based on actual HTML
        date_str = response.css(".meeting-date::text").get()
        time_str = response.css(".meeting-time::text").get()

        if not date_str or not time_str:
            return None

        # Handle time ranges
        if " - " in time_str:
            end_time_str = time_str.split(" - ")[1].strip()
            datetime_str = f"{date_str} {end_time_str}"

            try:
                return parse(datetime_str)
            except Exception as e:
                self.logger.warning(
                    f"Error parsing end datetime '{datetime_str}' at {response.url}: {e}"
                )
                return None

        return None

    def _parse_location(self, response):
        """
        Extract the meeting location information.

        Args:
            response (scrapy.Response): Response from meeting detail page

        Returns:
            dict: Dictionary with 'name' and 'address' keys
        """
        # TODO: Update selectors based on actual HTML
        name = response.css(".location-name::text").get(default="").strip()
        street = response.css(".location-street::text").get(default="").strip()
        city = response.css(".location-city::text").get(default="").strip()
        state = response.css(".location-state::text").get(default="").strip()
        zip_code = response.css(".location-zip::text").get(default="").strip()

        # Format the address
        address_parts = [street, city, f"{state} {zip_code}"]
        formatted_address = ", ".join(part for part in address_parts if part)

        return {
            "name": name or "TBD",
            "address": formatted_address,
        }

    def _parse_links(self, response):
        """
        Extract links to meeting documents (agendas, minutes, etc.).

        Includes default links plus any document links found on the page.

        Args:
            response (scrapy.Response): Response from meeting detail page

        Returns:
            list: List of dicts with 'href' and 'title' keys
        """
        new_links = self.links.copy()  # Copy the default links

        # TODO: Update selector based on actual HTML
        for link in response.css(".meeting-documents a"):
            href = link.css("::attr(href)").get()
            title = link.css("::text").get()

            if href and title:
                absolute_url = self.ensure_absolute_url(href)
                new_links.append({"href": absolute_url, "title": title.strip()})

        return new_links

    def ensure_absolute_url(self, url):
        """
        Convert relative URLs to absolute URLs.

        Args:
            url (str): URL (relative or absolute)

        Returns:
            str: Absolute URL
        """
        parsed_url = urlparse(url)
        if not parsed_url.scheme:
            return urljoin(self.base_url, url)
        else:
            return url
